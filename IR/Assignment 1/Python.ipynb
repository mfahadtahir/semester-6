{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dying-england",
   "metadata": {},
   "source": [
    "# Assignment 1 IR | Boolean Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-breast",
   "metadata": {},
   "source": [
    "## 1. Libraries Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "forty-handy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "import tkinter as tk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-poison",
   "metadata": {},
   "source": [
    "## 2. Reading StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "domestic-broadcast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readStopWords():\n",
    "    f = open(\"stopwords.txt\", \"r\")\n",
    "    stopWords = f.read().split()\n",
    "  \n",
    "    # making a Frozen Set as we want to use the \"Hashing\" part of it when we are processing files to ignore stopWords\n",
    "    stepWordsStem = set()\n",
    "    # We are stemming Stop words too to be able to remove all stems of a stopWord \n",
    "    # i.e if have is a stopword we want to ignore have, having, haven't and so on all stems of have \n",
    "    \n",
    "    for stopword in stopWords:\n",
    "        stepWordsStem.add(ps.stem(stopword))\n",
    "    stepWordsStem = frozenset(stepWordsStem)\n",
    "    f.close()\n",
    "    print(stopWords)\n",
    "    return stepWordsStem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-greene",
   "metadata": {},
   "source": [
    "## 3. Finding Words from Stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dental-climb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'is', 'the', 'of', 'all', 'and', 'to', 'can', 'be', 'as', 'once', 'for', 'at', 'am', 'are', 'has', 'have', 'had', 'up', 'his', 'her', 'in', 'on', 'no', 'we', 'do']\n",
      "Dictionary Length:  6763\n"
     ]
    }
   ],
   "source": [
    "def readFile(file):\n",
    "#     f = open(f\"./ShortStories/{file}.txt\", encoding=\"utf8\")\n",
    "    f=open('./ShortStories/{}.txt'.format(file), encoding='utf-8')\n",
    "    \n",
    "    #   Read and Lower Case \n",
    "    data = f.read().lower()    \n",
    "        \n",
    "    #   Removing Punctuations\n",
    "    data = data.translate(str.maketrans('','',string.punctuation))\n",
    "\n",
    "    # We have identified all unique characters and numbers and removed all others but alphabets       \n",
    "    # '9', '0', '—', 'i', 'u', 'q', '2', 'e', 'ª', 'z', 'y', '”', '8', 'd', 'v', 'o', 'm', 'f', 'b', '¨', '7', '1', 'a', '’', 'l', 'w', '§', 'j', '“', 'x', 'g', '©', 'p', '6', '3', 'r', 'c', 't', 'ã', '5', '´', '¯', '‘', 'h', '4', 's', 'k', 'n\n",
    "    data = data.translate(str.maketrans('','','1234567890©§“”‘’\"´¨¯—ªã'))\n",
    "\n",
    "    # Word to word checking of file and adding to Dictionary\n",
    "    \n",
    "#     for word in data.split():\n",
    "        \n",
    "#         word = ps.stem(word)\n",
    "#         if word not in stopWords:\n",
    "#             print(word, ps.stem(word))\n",
    "\n",
    "#             if word in dictionary.keys():\n",
    "#                 dictionary[word].add(file)\n",
    "#             else:\n",
    "#                 dictionary[word] = {file}\n",
    "    \n",
    "    for i, word in enumerate(data.split()):\n",
    "        word = ps.stem(word)\n",
    "        if word not in stopWords:\n",
    "\n",
    "            if word in dictionary.keys():\n",
    "                if file in dictionary[word].keys():\n",
    "                    dictionary[word][file].add(i)\n",
    "                else:\n",
    "                    dictionary[word][file] = {i}\n",
    "            else:\n",
    "                dictionary[word] = {file: {i}}\n",
    "\n",
    "    f.close()\n",
    "    return\n",
    "\n",
    "dictionary = {}\n",
    "ps = PorterStemmer()\n",
    "stopWords = readStopWords()\n",
    "# Run for Each File\n",
    "for x in range(1,51):\n",
    "    readFile(x)\n",
    "    \n",
    "    \n",
    "print(\"Dictionary Length: \", len(dictionary))\n",
    "# print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-allowance",
   "metadata": {},
   "source": [
    "## 4. Writing Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "approved-progressive",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"dictionary.txt\", \"w\")\n",
    "\n",
    "with open('dictionary.txt', 'w') as the_file:\n",
    "    for word in dictionary:\n",
    "        f.write('{}\\n'.format(word))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-trance",
   "metadata": {},
   "source": [
    "## 5. Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "alike-extent",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"invertedIndex.txt\", \"w\")\n",
    "\n",
    "with open('invertedIndex.txt', 'w') as the_file:\n",
    "    f.write('{')\n",
    "    for word in dictionary:\n",
    "        f.write('\"{}\": {},\\n'.format(word, list(dictionary[word].keys())))\n",
    "#         print(list(dictionary[word].keys()))\n",
    "    f.write('}')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-symbol",
   "metadata": {},
   "source": [
    "## 5. Positional Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "apparent-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"positionalIndex.txt\", \"w\")\n",
    "    \n",
    "with open('positionalIndex.txt', 'w') as the_file:\n",
    "    f.write('{')\n",
    "    for word in dictionary:\n",
    "#         print('\"{}\":\"{}\"\\n'.format(word, dictionary[word] ))\n",
    "        f.write(\"'{}':{},\\n\".format(word, dictionary[word] ))\n",
    "    f.write('}')\n",
    "\n",
    "    f.close()\n",
    "                       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-depression",
   "metadata": {},
   "source": [
    "## 6. Reading Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efficient-powell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 4, 6, 11, 20, 21, 23, 25, 26, 31, 34, 40, 44]\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "f = open(\"invertedIndex.txt\", \"r\")\n",
    "data = f.read()\n",
    "invertedIndex = ast.literal_eval(data)\n",
    "print(invertedIndex['beard'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-finish",
   "metadata": {},
   "source": [
    "## 7. Reading Positional Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "selected-installation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {737, 1978}, 2: {2074, 4442, 591}, 4: {752}, 6: {152}, 11: {205, 719}, 20: {36, 511}, 21: {134}, 23: {2145, 2882, 3641, 4382, 1151}, 25: {5758, 559}, 26: {972}, 31: {12}, 34: {490, 822}, 40: {674}, 44: {909}}\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "f = open(\"positionalIndex.txt\", \"r\")\n",
    "data = f.read()\n",
    "positionalIndex = ast.literal_eval(data)\n",
    "print(positionalIndex['beard'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-fortune",
   "metadata": {},
   "source": [
    "## 8. Normal Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "european-consent",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def normalQuery(query):\n",
    "\n",
    "    currentIndex = 0\n",
    "    resultDocs = set()\n",
    "    universalSet = set(range(1,51))\n",
    "    error = False\n",
    "#     For First Word\n",
    "    if query[currentIndex] == \"not\":\n",
    "        resultForWord = set(invertedIndex[ps.stem(query[currentIndex + 1])]) \n",
    "#       taking Not\n",
    "        resultDocs = resultDocs.union(universalSet - resultForWord)\n",
    "        currentIndex += 2\n",
    "    else:\n",
    "        resultDocs = resultDocs.union(set(invertedIndex[ps.stem(query[currentIndex])]))\n",
    "        currentIndex += 1\n",
    "              \n",
    "#   For Second Word\n",
    "\n",
    "    if len(query) > currentIndex:\n",
    "    \n",
    "#     Does the second word have AND or OR\n",
    "        if query[currentIndex] == \"and\":\n",
    "        \n",
    "#           Is the second word with a not ?\n",
    "            if query[currentIndex + 1] == \"not\":\n",
    "                resultForWord = set(invertedIndex[ps.stem(query[currentIndex + 2])])      \n",
    "#               taking Not\n",
    "                takingNot = universalSet - resultForWord\n",
    "#               Intersection Due to 'AND'\n",
    "                resultDocs = resultDocs.intersection(takingNot)\n",
    "                currentIndex += 3\n",
    "            else:\n",
    "                resultDocs = resultDocs.intersection(set(invertedIndex[ps.stem(query[currentIndex + 1])]))\n",
    "                currentIndex += 2\n",
    "\n",
    "                \n",
    "        elif query[currentIndex] == \"or\":\n",
    "            if query[currentIndex] == \"not\":\n",
    "                resultForWord = set(invertedIndex[ps.stem(query[currentIndex + 2])])\n",
    "                \n",
    "#               ----------------taking Not\n",
    "                resultDocs = resultDocs.union(universalSet - resultForWord)\n",
    "\n",
    "                currentIndex += 3\n",
    "            else:\n",
    "                resultDocs = resultDocs.union(set(invertedIndex[ps.stem(query[currentIndex + 1])]))\n",
    "                currentIndex += 2\n",
    "        else: \n",
    "            print(\"Invalid Query, kindly mention any of (AND, OR, NOT) operators between each word\")\n",
    "            error = True;\n",
    "            \n",
    "#   For Third Word\n",
    "\n",
    "    if len(query) > currentIndex and not error:\n",
    "        \n",
    "#     Does the third word have AND or OR\n",
    "        if query[currentIndex] == \"and\":\n",
    "        \n",
    "#           Is the third word with a not ?\n",
    "            if query[currentIndex + 1] == \"not\":\n",
    "                resultForWord = set(invertedIndex[ps.stem(query[currentIndex + 2])])      \n",
    "#               taking Not\n",
    "                takingNot = universalSet - resultForWord\n",
    "    \n",
    "#               Intersection Due to 'AND'\n",
    "                resultDocs = resultDocs.intersection(takingNot)\n",
    "                currentIndex += 3\n",
    "            else:\n",
    "#               union due to OR\n",
    "                resultDocs = resultDocs.intersection(set(invertedIndex[ps.stem(query[currentIndex + 1])]))\n",
    "                currentIndex += 2\n",
    "\n",
    "                \n",
    "        elif query[currentIndex] == \"or\":\n",
    "            if query[currentIndex] == \"not\":\n",
    "                resultForWord = set(invertedIndex[ps.stem(query[currentIndex + 2])])      \n",
    "#               taking Not\n",
    "                resultDocs = resultDocs.union(universalSet - resultForWord)\n",
    "\n",
    "                currentIndex += 3\n",
    "            else:\n",
    "#               union due to OR\n",
    "                resultDocs = resultDocs.union(set(invertedIndex[ps.stem(query[currentIndex + 1])]))\n",
    "                currentIndex += 2\n",
    "        else: \n",
    "            print(\"Invalid Query, kindly mention any of (AND, OR, NOT) operators between each word\")\n",
    "            error = True\n",
    "    \n",
    "    if(not error):\n",
    "        print(resultDocs)\n",
    "        return resultDocs\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-basement",
   "metadata": {},
   "source": [
    "## 9. Proximity Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "tamil-playing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{25, 11, 6, 22}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{6, 11, 22, 25}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def proximityQuery(word1, word2, k):\n",
    "    k = int(k)\n",
    "    qualified = set()\n",
    "    firstWordFiles = positionalIndex[ps.stem(word1)]\n",
    "    secondWordFiles = positionalIndex[ps.stem(word2)]\n",
    "    \n",
    "#     Finding Sets of each dictionary keys\n",
    "    word1Set = set(firstWordFiles)\n",
    "    word2Set = set(secondWordFiles)\n",
    "    \n",
    "#     Finding common keys\n",
    "    result = word1Set.intersection(word2Set)\n",
    "\n",
    "#     Finding compliment of keys with their respective sets to find uncommon keys \n",
    "    word1Set = word1Set - result\n",
    "    word2Set = word2Set - result\n",
    "    \n",
    "#     Removing the uncommon keys from word Files\n",
    "    for uslessfile in word1Set:\n",
    "        firstWordFiles.pop(uslessfile)\n",
    "\n",
    "    for uslessfile in word2Set:\n",
    "        secondWordFiles.pop(uslessfile)\n",
    "    \n",
    "#     Finding K distance files\n",
    "    for file in secondWordFiles:\n",
    "        for locationOfWord in secondWordFiles[file]:\n",
    "            result = set(range(locationOfWord - k - 1,locationOfWord + k + 1)).intersection(firstWordFiles[file])\n",
    "            if(len(result)):\n",
    "                qualified.add(file)\n",
    "\n",
    "    print(qualified)\n",
    "    return qualified\n",
    "\n",
    "proximityQuery('smiling', 'face', 3)\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-arbitration",
   "metadata": {},
   "source": [
    "## 9. Handling a Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "damaged-creator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"filling room /1\".lower().split()\n",
    "import sys\n",
    "# print(query)\n",
    "def handle_query():\n",
    "#     query = inputBox.get(\"1.0\",\"end-1c\")\n",
    "    query = input()\n",
    "    query = query.lower().split()\n",
    "    if query[len(query) - 1][0] == '/':\n",
    "        # It is a proximity Query\n",
    "        result = proximityQuery(query[0], query[1], query[2][1])\n",
    "#         result_box.delete(0.0, 'end')\n",
    "#         result_box.insert('end', result)\n",
    "        print(result)\n",
    "    else:\n",
    "        # It is a normal Query\n",
    "        result = normalQuery(query)\n",
    "#         result_box.delete(0.0, 'end')\n",
    "#         result_box.insert('end', result)\n",
    "        print(result)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "hungry-harris",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lover bullet /3\n",
      "{15}\n",
      "{15}\n"
     ]
    }
   ],
   "source": [
    "handle_query()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-campaign",
   "metadata": {},
   "source": [
    "## 10. Main Function GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "typical-palestinian",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/maxplanko89/anaconda3/lib/python3.8/tkinter/__init__.py\", line 1883, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"<ipython-input-13-b8f82868c207>\", line 5, in handle_query\n",
      "    query = inputBox.get(\"1.0\",\"end-1c\")\n",
      "TypeError: get() takes 1 positional argument but 3 were given\n"
     ]
    }
   ],
   "source": [
    "root = tk.Tk()\n",
    "root.geometry(\"600x400\")\n",
    "root.title('Boolean Retreival Model')\n",
    "\n",
    "\n",
    "tk.Label(root, text=\"Boolean Retreival Model\", font=(\"Roboto\", 20)).pack(pady=(5, 10))\n",
    "\n",
    "\n",
    "tk.Label(root, text=\"Search...\", font=(\"Roboto\", 12)).pack(pady=(10, 2))\n",
    "\n",
    "\n",
    "inputBox = tk.Entry(root, font=(\"Roboto\", 12), width=45)\n",
    "\n",
    "inputBox.pack()\n",
    "\n",
    "\n",
    "submitButton = tk.Button(root, text=\"SUBMIT\", width=40, command=handle_query).pack()\n",
    "\n",
    "\n",
    "tk.Label(root, text=\"Result Documents\", font=(\"Roboto\", 12)).pack(pady=(10, 2))\n",
    "\n",
    "root.bind('<Return>', handle_query)\n",
    "\n",
    "    # resultBox\n",
    "result_box = tk.Text(root, width=50, height=5, font=(\"Roboto\", 12), wrap='word')\n",
    "\n",
    "result_box.pack(pady=(0, 10))\n",
    "\n",
    "\n",
    "root.mainloop()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-replica",
   "metadata": {},
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infectious-shopping",
   "metadata": {},
   "source": [
    "## Finding All Characters used before translating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "electrical-kentucky",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'9', '0', '—', 'i', 'u', 'q', '2', 'e', 'ª', 'z', 'y', '”', '8', 'd', 'v', 'o', 'm', 'f', 'b', '¨', '7', '1', 'a', '’', 'l', 'w', '§', 'j', '“', 'x', 'g', '©', 'p', '6', '3', 'r', 'c', 't', 'ã', '5', '´', '¯', '‘', 'h', '4', 's', 'k', 'n'}\n"
     ]
    }
   ],
   "source": [
    "allchars = set()    \n",
    "for word in dictionary:\n",
    "        for letter in word:\n",
    "            allchars.add(letter)\n",
    "print(allchars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-bishop",
   "metadata": {},
   "source": [
    "## String Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "small-relations",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line:  my long strings big  sentance antisocial my different string\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import string \n",
    "\n",
    "\n",
    "    #   Read a Line and work on it and repeat\n",
    "line = \"my long string's big 9 sentance anti-social different 1str@ing!\"\n",
    "        \n",
    "    #   End of line Found\n",
    "\n",
    "line = line.lower()\n",
    "line = line.translate(str.maketrans('','',string.punctuation))\n",
    "line = line.translate(str.maketrans('','','1234567890'))\n",
    "line = line.translate(str.maketrans('','','“”‘’\"—'))\n",
    "\n",
    "# words = word_tokenize(line)\n",
    "\n",
    "print(\"Line: \", line)\n",
    "# print(\"Words: \", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-institute",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sapphire-separation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my  \t\t:\t my\n",
      "long  \t\t:\t long\n",
      "string  \t\t:\t string\n",
      "'s  \t\t:\t 's\n",
      "big  \t\t:\t big\n",
      "sentance  \t\t:\t sentanc\n",
      "that-make  \t\t:\t that-mak\n",
      "different  \t\t:\t differ\n",
      "1str  \t\t:\t 1str\n",
      "@  \t\t:\t @\n",
      "ing  \t\t:\t ing\n",
      "!  \t\t:\t !\n"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download()\n",
    "# importing modules\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(\"my long string's big sentance that-make different 1str@ing!\")\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for w in words:\n",
    "    print(w, \" \\t\\t:\\t\", ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-commitment",
   "metadata": {},
   "source": [
    "## StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "serial-assumption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a a\n",
      "is is\n",
      "the the\n",
      "of of\n",
      "all all\n",
      "and and\n",
      "to to\n",
      "can can\n",
      "be be\n",
      "as as\n",
      "once onc\n",
      "for for\n",
      "at at\n",
      "am am\n",
      "are are\n",
      "has ha\n",
      "have have\n",
      "had had\n",
      "up up\n",
      "his hi\n",
      "her her\n",
      "in in\n",
      "on on\n",
      "no no\n",
      "we we\n",
      "do do\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "f = open(\"stopwords.txt\", \"r\")\n",
    "stopwords = []\n",
    "while(1):\n",
    "    line = f.readline().strip()\n",
    "    if(not line):\n",
    "        break\n",
    "    stopwords.append(line)\n",
    "f.close()\n",
    "for psword in stopwords:\n",
    "    print(psword, ps.stem(psword))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-battery",
   "metadata": {},
   "source": [
    "## Removing StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-defensive",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"stopwords.txt\", \"r\")\n",
    "stopWords = f.read().split()\n",
    "for stopword in stopWords:\n",
    "    dictionary.pop(ps.stem(stopword))\n",
    "f.close()\n",
    "print(\"Dictionary Length: \", len(dictionary))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-restaurant",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Moving to Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bearing-shipping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary Length:  6789\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    " \n",
    "ps = PorterStemmer()\n",
    "\n",
    "def readFile(file):\n",
    "#     f = open(f\"./ShortStories/{file}.txt\", encoding=\"utf8\")\n",
    "    f=open('./ShortStories/{}.txt'.format(file), encoding='utf-8')\n",
    "    \n",
    "    #   Read and Lower Case \n",
    "    line = f.read().lower()    \n",
    "        \n",
    "    #   Removing Punctuations\n",
    "    line = line.translate(str.maketrans('','',string.punctuation))\n",
    "\n",
    "    # We have identified all unique characters and numbers and removed all others but alphabets       \n",
    "    # '9', '0', '—', 'i', 'u', 'q', '2', 'e', 'ª', 'z', 'y', '”', '8', 'd', 'v', 'o', 'm', 'f', 'b', '¨', '7', '1', 'a', '’', 'l', 'w', '§', 'j', '“', 'x', 'g', '©', 'p', '6', '3', 'r', 'c', 't', 'ã', '5', '´', '¯', '‘', 'h', '4', 's', 'k', 'n\n",
    "    line = line.translate(str.maketrans('','','1234567890©§“”‘’\"´¨¯—ªã'))\n",
    "        \n",
    "    for word in line.split():\n",
    "        word = ps.stem(word)\n",
    "        if word in dictionary.keys():\n",
    "            dictionary[word].add(file)\n",
    "        else:\n",
    "            dictionary[word] = {file}\n",
    "\n",
    "\n",
    "    f.close()\n",
    "    return\n",
    "\n",
    "dictionary = {}\n",
    "\n",
    "# Run for Each File\n",
    "for x in range(1,51):\n",
    "    readFile(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"Dictionary Length: \", len(dictionary))\n",
    "# print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "protected-clearing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{range(234, 237)}\n"
     ]
    }
   ],
   "source": [
    "print({range(234,237)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-terrace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
